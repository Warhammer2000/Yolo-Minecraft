{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Minecraft Object Detection Project\n",
        "\n",
        "Notebook for EDA, fine-tuning FCOS and YOLO, and benchmarking on the Minecraft mobs dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Roadmap\n",
        "\n",
        "1. Environment setup and configuration checks\n",
        "2. Dataset verification, EDA, and visualization\n",
        "3. Baseline inference with pretrained FCOS and YOLO models\n",
        "4. Fine-tuning experiments and metric tracking\n",
        "5. Comparative evaluation, video inference, and reporting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Environment bootstrap ---\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "SRC_DIR = PROJECT_ROOT / \"src\"\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Dataset Inspection & EDA\n",
        "\n",
        "> TODO: Load COCO annotations, validate splits, analyze class distribution, and visualize labeled samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset paths and annotation loading\n",
        "from src.data.dataset_checks import (\n",
        "    ensure_annotation_files_exist,\n",
        "    load_coco_annotations,\n",
        "    validate_image_annotation_counts,\n",
        "    ensure_class_coverage,\n",
        "    summarize_class_distribution,\n",
        ")\n",
        "from src.visualization.plots import plot_class_distribution, visualize_bboxes\n",
        "\n",
        "DATASET_ROOT = PROJECT_ROOT / \"datasets\" / \"minecraft\"\n",
        "annotation_paths = ensure_annotation_files_exist(DATASET_ROOT)\n",
        "\n",
        "train_annotations = load_coco_annotations(annotation_paths[\"train\"])\n",
        "val_annotations = load_coco_annotations(annotation_paths[\"val\"])\n",
        "test_annotations = load_coco_annotations(annotation_paths[\"test\"])\n",
        "\n",
        "ensure_class_coverage(train_annotations)\n",
        "ensure_class_coverage(val_annotations)\n",
        "ensure_class_coverage(test_annotations)\n",
        "print(\"Annotation files verified and class coverage confirmed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summaries for each split\n",
        "import pandas as pd\n",
        "\n",
        "split_summaries = []\n",
        "for split_name, annotations in (\n",
        "    (\"train\", train_annotations),\n",
        "    (\"val\", val_annotations),\n",
        "    (\"test\", test_annotations),\n",
        "):\n",
        "    num_images, num_ann = validate_image_annotation_counts(annotations)\n",
        "    split_summaries.append({\n",
        "        \"split\": split_name,\n",
        "        \"images\": num_images,\n",
        "        \"annotations\": num_ann,\n",
        "        \"ann_per_image\": num_ann / num_images if num_images else 0,\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(split_summaries)\n",
        "display(summary_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution plot\n",
        "class_counts = summarize_class_distribution(train_annotations)\n",
        "plot_class_distribution(class_counts, output_path=PROJECT_ROOT / \"artifacts\" / \"metrics\" / \"class_distribution.png\")\n",
        "class_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualise a sample from the test split\n",
        "from random import choice\n",
        "\n",
        "category_id_to_name = {c[\"id\"]: c[\"name\"] for c in test_annotations[\"categories\"]}\n",
        "image_info = choice(test_annotations[\"images\"])\n",
        "image_id = image_info[\"id\"]\n",
        "image_annotations = [ann for ann in test_annotations[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
        "\n",
        "image_path = DATASET_ROOT / \"test\" / \"images\" / image_info[\"file_name\"]\n",
        "output_path = PROJECT_ROOT / \"artifacts\" / \"inference\" / \"eda_test_sample.jpg\"\n",
        "visualize_bboxes(image_path, image_annotations, category_id_to_name, output_path=output_path)\n",
        "image_path, output_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation Plan\n",
        "\n",
        "- **Data pipeline**: maintain dual-format annotations (COCO + YOLO) and expose validation utilities for integrity checks.\n",
        "- **FCOS branch**: customise MMDetection config (`configs/fcos/fcos_minecraft.py`), fine-tune with mixed precision, and log metrics to `artifacts/fcos/`.\n",
        "- **YOLO branch**: leverage Ultralytics YOLOv8s with the generated YOLO labels, track outputs in `artifacts/yolo/`.\n",
        "- **Benchmarking**: consolidate metrics (mAP, mAP@50, precision, recall, F1, FPS) into comparison tables and figures.\n",
        "- **Inference layer**: produce image galleries and video runs for both detectors, persisting artefacts under `artifacts/inference/` and `artifacts/videos/`.\n",
        "- **Reporting**: compile visualisations, qualitative examples, and conclusions into a PDF report and summarise findings in the project README.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. FCOS Baseline & Fine-tuning\n",
        "\n",
        "> TODO: Configure MMDetection, run pretrained inference, launch fine-tuning, and log metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FCOS fine-tuning helper setup\n",
        "import torch\n",
        "from src.training.fcos_trainer import FCOSConfig, FCOSFineTuner\n",
        "\n",
        "fcos_config_path = PROJECT_ROOT / \"configs\" / \"fcos\" / \"fcos_minecraft.py\"\n",
        "fcos_work_dir = PROJECT_ROOT / \"artifacts\" / \"fcos\"\n",
        "\n",
        "checkpoint_candidates = sorted((PROJECT_ROOT / \"checkpoints\").glob(\"fcos*.pth\"))\n",
        "fcos_checkpoint = checkpoint_candidates[0] if checkpoint_candidates else None\n",
        "\n",
        "fcos_cfg = FCOSConfig(\n",
        "    config_path=fcos_config_path,\n",
        "    work_dir=fcos_work_dir,\n",
        "    checkpoint_path=fcos_checkpoint,\n",
        "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "\n",
        "print(f\"Using FCOS config: {fcos_config_path}\")\n",
        "print(f\"Work dir: {fcos_work_dir}\")\n",
        "print(f\"Pretrained checkpoint: {fcos_checkpoint}\")\n",
        "fcos_trainer = FCOSFineTuner(fcos_cfg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FCOS pretrained inference example\n",
        "if fcos_checkpoint is None:\n",
        "    raise FileNotFoundError(\"Place a pretrained FCOS checkpoint in the checkpoints/ folder (e.g. fcos_r50_caffe_fpn_gn-head_1x_coco.pth)\")\n",
        "\n",
        "fcos_sample_image = DATASET_ROOT / \"val\" / \"images\" / val_annotations[\"images\"][0][\"file_name\"]\n",
        "fcos_pretrained_output = PROJECT_ROOT / \"artifacts\" / \"inference\" / \"test_pretrained.jpg\"\n",
        "fcos_trainer.inference_on_image(fcos_sample_image, fcos_pretrained_output, checkpoint_path=fcos_checkpoint)\n",
        "fcos_pretrained_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO fine-tuning helper setup\n",
        "from src.training.yolo_trainer import YOLOConfig, YOLOFineTuner\n",
        "\n",
        "yolo_weights = PROJECT_ROOT / \"checkpoints\" / \"yolov8s.pt\"\n",
        "if not yolo_weights.exists():\n",
        "    print(\"Weights not found locally; Ultralytics will download yolov8s.pt on demand.\")\n",
        "    yolo_weights = \"yolov8s.pt\"\n",
        "\n",
        "yolo_cfg = YOLOConfig(\n",
        "    weights=str(yolo_weights),\n",
        "    data_yaml=DATASET_ROOT / \"data_coco.yaml\",\n",
        "    project_dir=PROJECT_ROOT / \"artifacts\" / \"yolo\",\n",
        "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        "    epochs=50,\n",
        "    imgsz=512,\n",
        "    batch=16,\n",
        "    name=\"minecraft_yolov8s\",\n",
        ")\n",
        "\n",
        "yolo_trainer = YOLOFineTuner(yolo_cfg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO pretrained inference example\n",
        "yolo_trainer.setup()\n",
        "yolo_sample_image = DATASET_ROOT / \"val\" / \"images\" / val_annotations[\"images\"][0][\"file_name\"]\n",
        "yolo_inference_dir = PROJECT_ROOT / \"artifacts\" / \"inference\" / \"yolo_val\"\n",
        "yolo_output_path = yolo_trainer.inference_on_image(yolo_sample_image, yolo_inference_dir)\n",
        "yolo_output_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO fine-tuning (run when ready)\n",
        "# yolo_trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO metrics summary (populate after training)\n",
        "from src.evaluation.metrics import load_yolo_metrics, summarise_yolo_results\n",
        "\n",
        "yolo_results_path = yolo_trainer.validation_metrics()\n",
        "if yolo_results_path.exists():\n",
        "    yolo_results_df = load_yolo_metrics(yolo_results_path)\n",
        "    yolo_summary = summarise_yolo_results(yolo_results_df)\n",
        "    yolo_summary\n",
        "else:\n",
        "    print(\"Run YOLO fine-tuning to generate artifacts/yolo/results.csv.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics comparison table\n",
        "from dataclasses import asdict\n",
        "\n",
        "comparison_rows = []\n",
        "if \"fcos_summary\" in locals():\n",
        "    comparison_rows.append({\"model\": \"fcos\", **asdict(fcos_summary)})\n",
        "if \"yolo_summary\" in locals():\n",
        "    comparison_rows.append({\"model\": \"yolov8s\", **asdict(yolo_summary)})\n",
        "\n",
        "if comparison_rows:\n",
        "    comparison_df = pd.DataFrame(comparison_rows)\n",
        "    metrics_csv = PROJECT_ROOT / \"artifacts\" / \"metrics\" / \"metrics_comparison.csv\"\n",
        "    metrics_csv.parent.mkdir(parents=True, exist_ok=True)\n",
        "    comparison_df.to_csv(metrics_csv, index=False)\n",
        "    display(comparison_df)\n",
        "else:\n",
        "    print(\"Populate fcos_summary and yolo_summary after training runs.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image inference utilities for qualitative comparison\n",
        "from itertools import islice\n",
        "\n",
        "def _resolve_fcos_checkpoint() -> Path | None:\n",
        "    candidates = [\n",
        "        fcos_cfg.work_dir / \"best_coco_bbox_mAP.pth\",\n",
        "        fcos_cfg.work_dir / \"epoch_12.pth\",\n",
        "        fcos_cfg.work_dir / \"latest.pth\",\n",
        "        fcos_checkpoint,\n",
        "    ]\n",
        "    for candidate in candidates:\n",
        "        if candidate and Path(candidate).exists():\n",
        "            return Path(candidate)\n",
        "    return None\n",
        "\n",
        "\n",
        "def generate_fcos_gallery(split: str = \"test\", limit: int = 5) -> list[Path]:\n",
        "    checkpoint = _resolve_fcos_checkpoint()\n",
        "    if checkpoint is None:\n",
        "        raise FileNotFoundError(\"No trained FCOS checkpoint found. Run training first.\")\n",
        "\n",
        "    output_dir = PROJECT_ROOT / \"artifacts\" / \"inference\" / \"fcos\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    gallery_paths = []\n",
        "    split_annotations = {\n",
        "        \"train\": train_annotations,\n",
        "        \"val\": val_annotations,\n",
        "        \"test\": test_annotations,\n",
        "    }[split]\n",
        "\n",
        "    for image_info in islice(split_annotations[\"images\"], limit):\n",
        "        image_path = DATASET_ROOT / split / \"images\" / image_info[\"file_name\"]\n",
        "        output_path = output_dir / image_info[\"file_name\"]\n",
        "        fcos_trainer.inference_on_image(image_path, output_path, checkpoint_path=checkpoint)\n",
        "        gallery_paths.append(output_path)\n",
        "    return gallery_paths\n",
        "\n",
        "\n",
        "def generate_yolo_gallery(split: str = \"test\", limit: int = 5, conf: float = 0.25) -> list[Path]:\n",
        "    if yolo_trainer.last_run_dir is None:\n",
        "        print(\"YOLO model has not been fine-tuned yet; using current weights for inference.\")\n",
        "\n",
        "    output_dir = PROJECT_ROOT / \"artifacts\" / \"inference\" / \"yolo\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    gallery_paths = []\n",
        "    split_annotations = {\n",
        "        \"train\": train_annotations,\n",
        "        \"val\": val_annotations,\n",
        "        \"test\": test_annotations,\n",
        "    }[split]\n",
        "\n",
        "    for image_info in islice(split_annotations[\"images\"], limit):\n",
        "        image_path = DATASET_ROOT / split / \"images\" / image_info[\"file_name\"]\n",
        "        output_path = yolo_trainer.inference_on_image(image_path, output_dir, conf=conf)\n",
        "        gallery_paths.append(output_path)\n",
        "    return gallery_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Video inference (execute after fine-tuning)\n",
        "minecraft_video = DATASET_ROOT / \"video.mp4\"\n",
        "fcos_video_output = PROJECT_ROOT / \"artifacts\" / \"videos\" / \"fcos_inference.mp4\"\n",
        "yolo_video_output = PROJECT_ROOT / \"artifacts\" / \"videos\" / \"yolo_inference.mp4\"\n",
        "\n",
        "# fcos_trainer.inference_on_video(minecraft_video, fcos_video_output)\n",
        "# yolo_trainer.inference_on_video(minecraft_video, yolo_video_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions to benchmark FPS on image batches\n",
        "import time\n",
        "from mmdet.apis import inference_detector\n",
        "\n",
        "\n",
        "def benchmark_fcos_fps(split: str = \"val\", limit: int = 50, score_thr: float = 0.25) -> float:\n",
        "    checkpoint = _resolve_fcos_checkpoint()\n",
        "    if checkpoint is None:\n",
        "        raise FileNotFoundError(\"FCOS checkpoint not available for FPS benchmark.\")\n",
        "\n",
        "    model = fcos_trainer._ensure_model(checkpoint)\n",
        "    split_annotations = {\n",
        "        \"train\": train_annotations,\n",
        "        \"val\": val_annotations,\n",
        "        \"test\": test_annotations,\n",
        "    }[split]\n",
        "\n",
        "    image_paths = [DATASET_ROOT / split / \"images\" / info[\"file_name\"] for info in islice(split_annotations[\"images\"], limit)]\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    for image_path in image_paths:\n",
        "        _ = inference_detector(model, str(image_path))\n",
        "    elapsed = time.perf_counter() - start\n",
        "    return len(image_paths) / elapsed if elapsed else 0.0\n",
        "\n",
        "\n",
        "def benchmark_yolo_fps(split: str = \"val\", limit: int = 50, conf: float = 0.25) -> float:\n",
        "    if yolo_trainer.model is None:\n",
        "        yolo_trainer.setup()\n",
        "    assert yolo_trainer.model is not None\n",
        "\n",
        "    split_annotations = {\n",
        "        \"train\": train_annotations,\n",
        "        \"val\": val_annotations,\n",
        "        \"test\": test_annotations,\n",
        "    }[split]\n",
        "\n",
        "    image_paths = [DATASET_ROOT / split / \"images\" / info[\"file_name\"] for info in islice(split_annotations[\"images\"], limit)]\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    for image_path in image_paths:\n",
        "        _ = yolo_trainer.model.predict(\n",
        "            source=str(image_path),\n",
        "            device=yolo_cfg.device,\n",
        "            imgsz=yolo_cfg.imgsz,\n",
        "            conf=conf,\n",
        "            save=False,\n",
        "            verbose=False,\n",
        "        )\n",
        "    elapsed = time.perf_counter() - start\n",
        "    return len(image_paths) / elapsed if elapsed else 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FCOS fine-tuning (run when ready)\n",
        "# fcos_trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FCOS metrics summary (populate after training)\n",
        "from src.evaluation.metrics import load_fcos_metrics, summarise_fcos_results\n",
        "\n",
        "fcos_log_path = fcos_trainer.export_metrics()\n",
        "if fcos_log_path.exists():\n",
        "    fcos_log_entries = load_fcos_metrics(fcos_log_path)\n",
        "    if fcos_log_entries:\n",
        "        fcos_summary = summarise_fcos_results(fcos_log_entries)\n",
        "        fcos_summary\n",
        "    else:\n",
        "        print(\"No validation entries found in log.json yet.\")\n",
        "else:\n",
        "    print(\"Run training to generate artifacts/fcos/log.json.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. YOLO Baseline & Fine-tuning\n",
        "\n",
        "> TODO: Prepare Ultralytics dataset YAML, run pretrained inference, fine-tune YOLOv8s, and capture metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comparative Evaluation & Reporting\n",
        "\n",
        "> TODO: Summarize metric tables, plot comparisons, run video inference, and export report artifacts.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
